{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mObFb0fGY4QS"
   },
   "source": [
    "# Credit Card Fraud Detection\n",
    "Anonymized credit card transactions labeled as fraudulent or genuine\n",
    "\n",
    "In this dataset, we have the feature 'Class,' which is what we're trying to predict. When 'Class' is equal to 1, it means the transaction is fraudulent, and when it's 0, it means the transaction is not fraudulent.\n",
    "\n",
    "The dataset comprises numerical features about the credit card transactions. We have features labeled V1 through V28, which are basically mathematical summaries of the transaction details obtained using a technique called PCA (Principal Component Analysis). Don't worry too much about what PCA is for now; just know that these features help us understand the transactions better.\n",
    "Credit card transaction datasets often contain a large number of features, such as transaction amount, merchant ID, location, time, and more. These features can result in high-dimensional data, making it challenging to analyze and build models effectively. PCA can also be used to anonymize sensitive data, such as credit card transaction details.\n",
    "\n",
    "Two features, 'Time' and 'Amount', are not transformed using PCA. 'Time' tells us how much time has passed since the first transaction, but we won't use it in our training. 'Amount' tells us the transaction amount, and we will do some transformations on it to make it more useful for our analysis.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax6LxoKLbKAf"
   },
   "source": [
    "## Libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiXi_6w1YlXl"
   },
   "outputs": [],
   "source": [
    "from keras.src.metrics.confusion_metrics import FalsePositives\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.metrics import FalseNegatives, FalsePositives, TrueNegatives, TruePositives, Precision, Recall, BinaryAccuracy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4--0aItb4Oi"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqEeKwVX9ByY"
   },
   "source": [
    "Load the data from the csv file creditcard.csv into a pandas dataframe. Use the variable df for the dataframe.\n",
    "Display the first 5 records in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRrYRxrJb6mQ"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j3eEsBIcMod"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oID2Q3lgnUx"
   },
   "source": [
    "# Class distribution in the dataset\n",
    "Use pandas' `value_counts` to determine the Class distribution in the dataset. Then, create a [countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html) using seaborn to visualize the class distribution. Based on your analysis, is the dataset balanced or imbalanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IycebKqWczcN"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCP9cdfndYF_"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg2uGdRnjQcV"
   },
   "source": [
    "## Data processing\n",
    "Remove feature 'Time' from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQhedyHwztHp"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_wbHwjth5Qr"
   },
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZOpsXq7ZDZr"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Class\"], axis=1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_f2ISCx9pqN"
   },
   "source": [
    "Split the dataset in a trainingset and a testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwEnoZgA2EnJ"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JRj6ynG2eQy"
   },
   "source": [
    "How many features are there in the (training) data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0JpxygA2kls"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUYaKHSoz6h-"
   },
   "source": [
    "## Data processing\n",
    "What is the range of the variable 'Amount'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciDIlfoH97wf"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-Cgp4jp-Am5"
   },
   "source": [
    "Reduce this wide range using Normalization or Standardization. It is important to note that the scaler should be fitted to the training and the test data, rather than to the entire dataset, in order to prevent leakage of information from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SFdysJLyNYE"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiviCdOF1CR6"
   },
   "source": [
    "Neural networks can't handle pandas dataframes, but the min_max_scaler output is a numpy array. Numpy array are usable as input for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD-yCjbSzY1u"
   },
   "source": [
    "## Build a binary classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9vtzuEZ3X0f"
   },
   "source": [
    "Create a neural network with an input layer with 32 neurons and ReLU activation, three hidden layers, each with 32 neurons and ReLU activation.\n",
    "And finally, the output layer. How many neurons do you need for a binary classifcation problem? Which activation function will you use?\n",
    "\n",
    "Create a summary of the network. How many trainable parameters are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c61idHzOanoE"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnK5JV2B23mG"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VhAE1d0alTJ"
   },
   "source": [
    "## Compile the model\n",
    "The next step is to compile the neural network. Provide two functions: a loss function and an optimizer. The loss function evaluates the set of weights by how well the model predicts, while the optimizer modifies the weights to reduce the loss. The list of metrics below is also provided to the compile() method. Metric values are displayed during fit() (training) and logged to the History-object returned by fit().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toSIWWIx64Ku"
   },
   "outputs": [],
   "source": [
    "from keras.src.metrics import BinaryAccuracy\n",
    "metrics = [\n",
    "  FalseNegatives(name=\"fn\"),\n",
    "  FalsePositives(name=\"fp\"),\n",
    "  TrueNegatives(name=\"tn\"),\n",
    "  TruePositives(name=\"tp\"),\n",
    "  Precision(name=\"precision\"),\n",
    "  Recall(name=\"recall\"),\n",
    "  BinaryAccuracy(name=\"accuracy\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkPkji294gK5"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APwWpZAU_rf1"
   },
   "source": [
    "**Remark:** When you pass the strings 'accuracy' or 'acc' to the metrics for , we convert this to one of tf.keras.metrics.BinaryAccuracy, tf.keras.metrics.CategoricalAccuracy, tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. https://stackoverflow.com/questions/65023353/difference-between-keras-metrics-accuracy-and-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QoOt_Q9_vmW"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Call fit() to train the model. Use batches with batch_size 2048 and iterate 10 times over the entire dataset. Because you are coping with an imbalanced dataset, it's a good idea to provide different weights for each class when using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_D__U1EAi9D"
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 0.5, 1: 5.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27Q5AZdq_8z9"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVbWlLv_4otV"
   },
   "source": [
    "## Evaluate the model\n",
    "Evaluate the model on the test data using the methode 'evaluate'.\n",
    "Display the confusion matrix for the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghl_q-jV5BGf"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1vdAhHD6Q22"
   },
   "outputs": [],
   "source": [
    "# Add the code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
